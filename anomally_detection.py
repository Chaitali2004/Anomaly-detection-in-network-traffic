# -*- coding: utf-8 -*-
"""Anomally_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jRoMnTvmVTv6dn42xgGkoWADXq_-gB4t
"""

import pandas as pd

# Define your feature list (42 + label)
features = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',
    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',
    'logged_in', 'num_compromised', 'root_shell', 'su_attempted',
    'num_root', 'num_file_creations', 'num_shells', 'num_access_files',
    'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',
    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',
    'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',
    'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',
    'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
    'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',
    'dst_host_srv_serror_rate', 'dst_host_rerror_rate',
    'dst_host_srv_rerror_rate', 'label']
print(" Dataset loaded successfully")

# Step 2: Load the dataset
from google.colab import drive
drive.mount('/content/drive')
file_path = "/content/drive/MyDrive/Colab Notebooks/kddcup.data.corrected"
df = pd.read_csv(file_path, names=features, header=None)

print(df.head())

from google.colab import drive
drive.mount('/content/drive')

#  Step 2: Clean the labels FIRST
df['label'] = df['label'].str.strip('.')  # strip the dots

#  Step 3: Convert label to binary
df['binary_label'] = df['label'].apply(lambda x: 0 if x == 'normal' else 1)

# Step 4: Print verification
print(" Data loaded successfully")
print(df[['label', 'binary_label']].head())
print("Shape of dataset:", df.shape)
print("Attack class counts:\n", df['binary_label'].value_counts())

#  Check unique labels
output = df['label'].values
labels = set(output)
print('The different type of output labels are:', labels)
print('='*100)
print('No. of different output labels are:', len(labels))

# Step 5: Clean further
print(" Checking nulls:\n", df.isnull().sum())
df = df.drop_duplicates()
print(" Duplicates removed")
print(" Data types:\n", df.dtypes)

import joblib

y = joblib.load('/content/drive/My Drive/Colab Notebooks/data/y.pkl')
print(" y loaded successfully. Shape:", y.shape)

X_scaled = joblib.load('/content/drive/My Drive/Colab Notebooks/data/X_scaled.pkl')
y = joblib.load('/content/drive/My Drive/Colab Notebooks/data/y.pkl')

#train isolation forest model
from sklearn.ensemble import IsolationForest

iso_model = IsolationForest(contamination=0.1, random_state=42)
iso_model.fit(X_scaled)

# Predict (-1 = anomaly, 1 = normal)
preds = iso_model.predict(X_scaled)

# Convert to 0/1 (to match y)
preds = [1 if p == -1 else 0 for p in preds]

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

print(" Classification Report:\n", classification_report(y, preds))
print(" Confusion Matrix:\n", confusion_matrix(y, preds))

# Confusion Matrix Plot
sns.heatmap(confusion_matrix(y, preds), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Isolation Forest')
plt.show()

"""Autoencoder for Anomally Detection"""

# build autoencoders
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

input_dim = X_scaled.shape[1]
encoding_dim = 32  # compression factor

input_layer = Input(shape=(input_dim,))
encoder = Dense(64, activation="relu")(input_layer)
encoder = Dense(encoding_dim, activation="relu")(encoder)
decoder = Dense(64, activation='relu')(encoder)
decoder = Dense(input_dim, activation='sigmoid')(decoder)

autoencoder = Model(inputs=input_layer, outputs=decoder)
autoencoder.compile(optimizer='adam', loss='mse')

#Train the Autoencoder ONLY on normal traffic
X_train = X_scaled[y == 0]  # Use only normal data

history = autoencoder.fit(X_train, X_train,
                          epochs=20,
                          batch_size=256,
                          shuffle=True,
                          validation_split=0.2,
                          verbose=1)

X_pred = autoencoder.predict(X_scaled)

import numpy as np
mse = np.mean(np.power(X_scaled - X_pred, 2), axis=1)


threshold = np.percentile(mse, 95)
y_pred = (mse > threshold).astype(int)


threshold = np.percentile(mse, 95)  # Adjust if needed
y_pred = (mse > threshold).astype(int)

#choose threshold and predict anomalies
threshold = np.percentile(mse, 95)  # Adjust if needed
y_pred = (mse > threshold).astype(int)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

print(classification_report(y, y_pred))

sns.heatmap(confusion_matrix(y, y_pred), annot=True, fmt='d', cmap='YlOrRd')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Autoencoder Confusion Matrix')
plt.show()

#ROC-AUC Curve
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

fpr, tpr, _ = roc_curve(y, mse)   # `mse` is reconstruction error
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, label='AUC = %0.2f' % roc_auc)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Autoencoder')
plt.legend()
plt.grid(True)
plt.show()

import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.metrics import classification_report

# Take a subset for faster computation
subset_size = 10000
idx = np.random.choice(X_scaled.shape[0], subset_size, replace=False)
X_small = X_scaled[idx]
y_small = y[idx]

# Isolation Forest
iso = IsolationForest(contamination=0.1, random_state=42, n_jobs=-1)
y_iso = iso.fit_predict(X_small)
y_iso = np.where(y_iso == -1, 1, 0)
print("Isolation Forest:\n", classification_report(y_small, y_iso))

#Feature Importance in Anomalies
anomaly_data = X_scaled[y_pred == 1]  # predicted anomalies
avg_feature_values = np.mean(anomaly_data, axis=0)

plt.figure(figsize=(12,4))
plt.plot(avg_feature_values)
plt.title("Average Feature Values in Anomalies")
plt.xlabel("Feature Index")
plt.ylabel("Mean Scaled Value")
plt.grid(True)
plt.show()
from keras.models import load_model

"""## PHASE 3: t-SNE or PCA Visualization"""

import numpy as np

# Define the number of rows you want to sample
subset_size = 5000

# Randomly pick row indices
idx = np.random.choice(X_scaled.shape[0], subset_size, replace=False)
X_subset = X_scaled[idx]
y_subset = y[idx]
y_pred_subset = y_pred[idx]

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)
X_2d = tsne.fit_transform(X_subset)

#plot actual labels
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_subset, cmap='coolwarm', s=3)
plt.title('t-SNE Visualization of Real Labels (0=Normal, 1=Anomaly)')
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.grid(True)
plt.show()

# Plot Autoencoder Predictions
plt.figure(figsize=(8,6))
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_pred_subset, cmap='viridis', s=3)
plt.title('t-SNE Visualization of Autoencoder Predictions')
plt.xlabel("Component 1")
plt.ylabel("Component 2")
plt.grid(True)
plt.show()